<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Project: Conspiracy Theories - Krishna Shah</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <h1>Project: Conspiracy Theories</h1>
    <h2>By: Krishna Shah</h2>
    <p>Date: 15-Oct-2023</p>

    <hr />
    
    <p><strong>I recieved assistance from: N/A</strong></p>

    <p><strong>I assisted: N/A</strong></p>

    <hr />

    
    <p><strong>The Basics of Recommendation Algorithms --></strong> Recommendation algorithms are meant to improve a certain marketing metric by using systematic and targeted approaches to help the right content reach and appeal to the right audience whether that is the click-through rate of a video or the views of an ad. Any internet content can be promoted using this approach. Amazon uses this to promote products; Google uses this to promote ads from third parties; Twitter (now X), Instagram, Youtube use this to promote content to get user retention; and Medium uses this to promote articles and increase read time. Based on the platform and the medium being advertised, there are different metrics at play like watch-time, user retention, returning users, read time, interaction (for a website/ad), likes, shares, etc. The basic principle is to mathematically optimize (minimize or maximize) any one or combination of the metrics. Many algorithms use a deep neural network with trillions of hyperparameters to understand the user preferences of millions of individuals and even distinct groups of users to provide them with the best possible content to increase retention and engagement. Finally, there are two major forms of recommendation an algorithm can give: collaborative filtering and content-based filtering. In collaborative filtering, the algorithm will attempt to assign you into a group based on your preferences and will recommend content that is prefered in the group but not viewed by you. This way, if you belong in that group and share interests, you will find the recommended content appealing. For example, if I frequently search for Nike running shoes on Google, it will put me in a runner's groups and start suggesting other running shoes that other runners commonly buy. Since the interests of that user are shared with other users, the preferences of other users can be suggested to others within the group to promote engagement. There is also content-based filtering that attempts to find you content similar to ones you have shown interest in. For example, if you show interest in USB-C chargers on Amazon, it will constantly suggest different desk chargers, fast chargers, and even wireless chargers since they are relatively similar to the original item you searched (i.e. showed interest in).</p>
    <img src="early-stages-flywheel.gif" alt="Cycle" class="center">
    <img src="alg1.png" alt="ALG1" class="center">
    <img src="alg2.jpg" alt="ALG2" class="center">
    <p><strong>The Feedback Loop --></strong> Once the algorithm observes the general preference of the viewer, it will immediately attempt to interpret the audience preferences and dislikes to start target content delivery. This process is very fast and can even span a single click the user makes on a video because user retention--especially for new users is crucial for any platform. It is due to these algorithms that misinformation is prevalent because any ephemeral interest the user expresses in conspiracies is reciprocated in a larger magnitude by the algorithm, leading to a positive feedback loop. This makes the user think that the conspiracy is supported by many people despite the content being curated by the algorithm. For example, opening YouTube on a cache and cookie free system shows a series of suggestions that are most common in the general public. Searching for a video of your choice--like "Veritasium"-- automatically places you in a group of viewers that enjoys knowledgable, science content. Furthermore, clicking on a video solidifies this grouping. If you now return to the YouTube homepage, all the common/generic suggestions have been replaced by science/physics content and a majority will be Veretasium (or associated with Veritasium) related content. Such rapid estimation of user preferences is dangerous and can lead to saturation of content since the user continously shows interest in the YouTUbe suggestions, thus YouTUbe continues to provide content similar to the ones the user prefers. This will continue to narrow the user preference if no "fuzzy" or "multi-shot" technique is employed. For example, after a month of watching science videos--where a majority are physics related since that is what YouTube reccomended--you will cease to see chemistry or biology content since--despite your initial interest being in the broad sciece category--was quickly narrowed down by the algorithm to be physics and the user and the algorithm were stuck in a positive feedback loop. Understanding this problem, YouTUbe introduced a feature that would "mix up" and refresh your suggestions with content that is more distantly related to your original preferences. </p> 
    <p><strong>Conspiracy Theories & Algorithmic Amplification --></strong> There are two components that make algorithmic amplification of misinformation dangerous: algorithms solely consider monetary gain when optimizing and suggesting content, and provocative content that aligns with seeds of misinformation in the user is more prefered by the user than facutal information. Putting these together, provocative and catchy content will attract more users which will make the algorithm suggest similar content, hence nurturing the seeds of misinformation in a bubble of homogenous content allowing it to grow into a firm belif. For example, between "The Truth about Climate Change: Total Destruction" or "Climate Change: A Government Hoax" people will tend to evade the first title because it blames humanity (including the user) for climate change and the second one will be selected because it is more provocative and often spread as a conspiracy. Selecting that single misinforming video will prompt the algorithm to quickly suggest other similar content and will completely avoid content that conflicts with the views of the original video. This clouds the viewer in misinformed views that the viewer will continue to watch. Additionally, the homogenity of the content suggests to the viewer that there is "so much research/support" for the idea and that other ideas simply are not as popular/supported. At this point, the audience has converted from a mildly skeptical to a total believer in the idea.</p>
    <p><strong>Platform Incentives --></strong> Depending on the platform, there are many incentives that the company uses as a metric. The sole requirement is the ability for the metric to be objective and numerical in nature. This could be clicks on an ad, views, likes, watch time, returning user count, subscribers, etc. Usually, companies will combine several metrics that have proven to increase profit by means of advertisement or purchase and will use a formula to combine them to promote a net increase in profit. For example, Google Ads and YouTube are ad-driven platforms that benefit off of ads given to the user. This also goes for music streaming platforms like Spotify. Shopping platforms like Amazon, Walmart, etc. will use user information to promote enticing products to promote a purchase. Such numerically and profit-driven algorithms cannot factor in subjective and nuanced ideas like the veracity of the claims of a product or content and the social and political impact of the product, which if not managed by humans, can lead to misinformation and radical political ideas. </p>
    <p><strong>Potential for Exploitation --></strong> There are many ways people have found to improve their content, misinformed or not, to spread by better understanding these algorithms. These algorithms, like YouTube, tend to also parse content information, not just user information. This means that it must parse titles, text, video, audio, etc. For text, a common method of parsing this is to use self-attention with a Query, Key, Value inputs to extract key words from a comparision table or high-dimentional cluster like t-SNE word plots. Using more keywords allows the given content to potentially match with more related content and reach a wider audience. For example, using the title "Single-Shot Self-Attention based Text Token Transformer by OpenAI Defeats BERT" appeals itself to a small crowd of people knowledgable and interested in AI. Rephrasing that to say "Genius AI ChatGPT Beats the Best AI and Can Talk to Humans!" contains many key words like "ChatGPT" that appeal to a broader audience and allows the algorithm to suggest it to more people. Thumbnails are also a source of great exploitation since a technique like click-bait can be applied. Using vibrant and alarming colors quickly grabs the attention of any wandering eyes in search of content. Using partially correct--or blatantly incorrect--statements will convince the viewer that the content contains better and more interesting information than it actually does. This leads the user to click into the video and begins the feedback loop discussed earlier. For example, suggesting that "President John Doe Waged War on Country X for a Soda Can!!!" is very exagerated when the reality yeilds a title like this: "President John Doe Closes Borders Due to The Illegal Import of Soda". The first title, however, is far more appealing with a highly provocative thumbnail. If exploiters understand the primary audience on the platform and the type of content they prefer, they will use that to specifically target the misinformation on the platform. For example, they might use gender or age to target groups by using key words that the algorithm commonly suggests to a particular gender, locale, or age group. </p> 


    <hr>

    <h2>Sources: </h2>
    <ul>
      <li><a href="https://www.shaped.ai/blog/explore-vs-exploit" target="_blank">Shaped.AI</a></li>
      <li><a href="https://builtin.com/data-science/recommender-systems" target="_blank">BuiltIn</a></li>
      <li><a href="https://www.brookings.edu/articles/how-do-recommender-systems-work-on-digital-platforms-social-media-recommendation-algorithms/#:~:text=As%20noted%20above%2C%20deep%20learning,liking%20or%20commenting%20on%20it." target="_blank">Brookings</a></li>
      <li><a href="https://medium.com/@zulkarnain.prastyoumb23093/what-is-modeling-for-structured-data-recommender-system-f8c08f82192d" target="_blank">Zulkarnain Prastyo - Medium</a></li>
      <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-66840-2_21" target="_blank">A Comparative Study Between K-Nearest Neighbors and K-Means Clustering Techniques of Collaborative Filtering in e-Learning Environment</a></li>
      <li><a href="https://sparktoro.com/blog/who-will-amplify-this-and-why/" target="_blank">SparkToro</a></li>
    
    <br>
    <div><button onclick="history.back()">Go Back</button>    <a href="https://codermerlin.com/users/krishna-shah/"><button>Home</button></a></div>
  </body>
</html>   
